{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13876959-877d-477c-a7a2-b5b55c72a4da",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a695be5c-0514-4827-90ef-75afcf96216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import progressbar\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import faiss\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7cd78-5d3f-4ead-a813-c6d8a17e9273",
   "metadata": {},
   "source": [
    "## Implementation of incident search and RCA (with extracted symptoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4692f-31c9-42c8-ac93-5985867ec258",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1 ) Collecting Entities to create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31691861-ad16-44f6-a63f-9c739bef8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cso_df = pd.read_csv('./CSO_data/'+ 'CSO_entities_ensembled_Sign_bart-large.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = './CSO_data/CSO_all_scraped_Sign.json'\n",
    "with open(json_file, 'r') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbc43df5-c9c4-4d66-9734-30399c146dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "model = RobertaModel.from_pretrained('roberta-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "030e40b1-e7b2-4893-9dd3-8f3e8fd80360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Roberta embeddings for the outage symptoms\n",
    "\n",
    "entities1 = ['symptom'] #, 'customer_impacts', 'short_description', 'description', 'remediations', 'root_cause', 'short_term_fix']\n",
    "sent_embed = {}\n",
    "\n",
    "def embed_sentences():\n",
    "    global sent_embed\n",
    "    \n",
    "    i = 0\n",
    "    for cso in cso_df['cso_number']:\n",
    "        for entity in entities1:\n",
    "            try:\n",
    "                text = cso_df[cso_df['cso_number'] == cso][entity].iloc[0]\n",
    "                text = text.strip()\n",
    "                input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_ids)\n",
    "                    last_hidden_states = outputs[0]\n",
    "                \n",
    "                token_embeddings = torch.squeeze(last_hidden_states, dim=0)\n",
    "                sentence_embed = torch.mean(token_embeddings, axis=0).numpy()\n",
    "                sent_dict = {'cso': cso, 'sent': text, 'embed': sentence_embed, 'tag': entity}\n",
    "                sent_embed[i] = sent_dict\n",
    "                i += 1\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7e99a7d4-9611-4615-8c11-e01b47fb7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d781ca0f-0404-4672-84b7-08955dd99be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8721d16a-e173-4b03-ad82-2204bf4dc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sentence embeddings (takes time)\n",
    "def save_sent_embeddings(path):\n",
    "    sent_ = {}\n",
    "    for i in sent_embed:\n",
    "        sent_[i] = sent_embed[i]\n",
    "        sent_[i]['embed'] = sent_embed[i]['embed'].tolist()\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(sent_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "76dc3330-e351-4bf1-ad23-076137c5b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sent_embeddings('Roberta_sent_embeddings_ESRO.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "491dd4e9-b822-429f-b60d-aadd42b66960",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split sentence embedding and store it'\n",
    "def split_save_sent_embeddings(data_dict, folder_name, file_generic):\n",
    "    l = len(data_dict.keys())\n",
    "    size = int(l*(0.01))\n",
    "    count = 0\n",
    "    temp = {}\n",
    "\n",
    "    for key in data_dict:\n",
    "        temp[key] = data_dict[key]\n",
    "        if isinstance(data_dict[key]['embed'], np.ndarray):\n",
    "            temp[key]['embed'] = data_dict[key]['embed'].tolist()\n",
    "        else:\n",
    "            temp[key] = data_dict[key]\n",
    "        count += 1\n",
    "        if (count%size == 0):\n",
    "            file_name = folder_name + '/' + file_generic + '_' +  str(count-size) + '_' + str(count) + '.json'\n",
    "            with open(file_name, 'w') as f:\n",
    "                json.dump(temp, f)\n",
    "            temp = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1fd99096-739f-4de6-80a6-bd3eb66b7fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:05] |********************************* | (ETA:   0:00:00) "
     ]
    }
   ],
   "source": [
    "split_save_sent_embeddings(sent_embed, 'FAISS - search/Roberta_sent_embeddings_ESRO', 'sent_embed')\n",
    "# already saved, load the same as save or save different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "512f93f0-7dac-45c1-8074-4df1314a2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentence embeddings which was stored in a single file (takes time)\n",
    "def load_sent_embeddings(path):\n",
    "    global sent_embed\n",
    "    sent_embed = {}\n",
    "    with open(path, 'r') as f:\n",
    "        sent_embed = json.load(f)\n",
    "    l = len(sent_embed.keys())\n",
    "\n",
    "    for i in sent_embed:\n",
    "        sent_embed[i]['embed'] = np.asarray(sent_embed[i]['embed'], dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fd85c81c-8199-486c-b1a3-7641f4da7820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
     ]
    }
   ],
   "source": [
    "load_sent_embeddings('Roberta_sent_embeddings_ESRO.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "31f3bf67-ccbd-43b0-b7cb-74f5a96a48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentence embeddings which were splitted (less time)\n",
    "def load_split_save_sent_embeddings(folder_name):\n",
    "    sent_embed = {}\n",
    "    sent = {}\n",
    "    print('reading files')\n",
    "    print(folder_name)\n",
    "    l = len(os.listdir(folder_name))\n",
    "    count = 0\n",
    "\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        print(folder_name + '/' + file)\n",
    "        with open(folder_name + '/' + file, 'r') as f:\n",
    "            sent = {**sent, **json.load(f)}\n",
    "        count += 1\n",
    "    \n",
    "    print('converting list to numpy array')\n",
    "    l = len(sent.keys())\n",
    "    count = 0\n",
    "\n",
    "    for i in sent:\n",
    "        sent_embed[int(i)] = sent[i]\n",
    "        sent_embed[int(i)]['embed'] = np.asarray(sent[i]['embed'], dtype = 'float32')\n",
    "        count += 1\n",
    "\n",
    "    return sent_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7d383-6503-4605-aecb-6e1d14d4eba3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## just run this line to load already calculated sentence embeddings in the directory sent_embed\n",
    "sent_embed_sign = load_split_save_sent_embeddings('./FAISS - search/Roberta_sent_embeddings_ESRO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc142584-a83c-4a18-913d-726cddae9c5d",
   "metadata": {},
   "source": [
    "#### 2 ) building the FAISS Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0fa9fa42-a187-4107-b385-5f913bb8f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numpy_array(s):\n",
    "    nb = len(s.keys())\n",
    "    xb = []\n",
    "    for i, k in enumerate(s.keys()):\n",
    "        xb.append(s[k]['embed'])\n",
    "        \n",
    "    return np.array(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "20bebac1-a09d-4558-bc7c-c5dae87cd06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numpy arrays of the sentence embeddings\n",
    "xb = create_numpy_array(sent_embed_sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "edf72dc3-9ff4-4060-9caf-2ecb75a2dae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 1024)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a4d161c8-b80b-4958-a806-b3538ef7d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(xb):\n",
    "    global xb_normalized\n",
    "    xb_normalized = deepcopy(xb)\n",
    "    faiss.normalize_L2(xb_normalized)\n",
    "    \n",
    "normalize(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3a156d05-3376-45eb-9cf9-70c187b08f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building FAISS index\n",
    "d = 1024\n",
    "Index_L2 = faiss.IndexFlatL2(d)\n",
    "Index_IP = faiss.IndexFlatIP(d)\n",
    "\n",
    "\n",
    "Index_L2.add(xb)\n",
    "Index_IP.add(xb_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c5165e14-ebd6-4000-809c-c2be357e2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faiss Index top-k search for a given CSO\n",
    "def find_top_k_similar(xq, k, basis = 'both'):\n",
    "    xq_normalized = deepcopy(xq)\n",
    "    faiss.normalize_L2(xq_normalized)\n",
    "    \n",
    "    D_L2, I_L2 = Index_L2.search(xq, k)\n",
    "    D_IP, I_IP = Index_IP.search(xq_normalized, k)\n",
    "    if basis == 'L2':\n",
    "        return D_L2, I_L2\n",
    "    if basis == 'IP':\n",
    "        return D_IP, I_IP\n",
    "    return D_L2, I_L2, D_IP, I_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2a60afd6-9dd3-48dc-a743-2abc3af3d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a text representing the symptom of an outage, rank the top-k symptoms\n",
    "def rank_cso(query):    \n",
    "    ranked_cso_dict = dict()\n",
    "    \n",
    "    input_ids = torch.tensor(tokenizer.encode(query, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs[0]\n",
    "\n",
    "    token_embeddings = torch.squeeze(last_hidden_states, dim=0)\n",
    "    sentence_embed = torch.mean(token_embeddings, axis=0).numpy().reshape(1,-1)\n",
    "    k = 12\n",
    "\n",
    "    D_IP, I_IP = find_top_k_similar(sentence_embed, k, 'IP')  ## shape (nq, k)\n",
    "\n",
    "    for i in range(1):\n",
    "        for j in range(k):\n",
    "            cso_sent_dict = sent_embed_sign[I_IP[i,j]]\n",
    "            \n",
    "            if cso_sent_dict['cso'] in ranked_cso_dict.keys():\n",
    "                ranked_cso_dict[cso_sent_dict['cso']]['score'] += D_IP[i, j]\n",
    "                ranked_cso_dict[cso_sent_dict['cso']]['sent'].append(cso_sent_dict['sent'])\n",
    "            else:\n",
    "                temp = {'score': D_IP[i, j], 'cso':cso_sent_dict['cso'], 'sent': [cso_sent_dict['sent']]}\n",
    "                ranked_cso_dict[cso_sent_dict['cso']] = temp\n",
    "    \n",
    "    return dict(sorted(ranked_cso_dict.items(), key=lambda x: x[1]['score'], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b434c2-1928-4a05-94cd-bd3fa1046249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faiss_rank(query):\n",
    "    t2 = rank_cso(query)\n",
    "    cso_dict = {}\n",
    "    for i in t2:\n",
    "        cso_dict[int(i)] = t2[i]['score']\n",
    "    return cso_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580b84a",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b38454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_stripper(code):\n",
    "    return BeautifulSoup(code).get_text().replace('\\r',' ').replace('\\xa0',' ').replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f932bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cso_from_dict(cso_dict1, exception_cso_number):\n",
    "    cso_dict = cso_dict1.copy()\n",
    "    if int(exception_cso_number) in cso_dict.keys():\n",
    "        del cso_dict[int(exception_cso_number)]\n",
    "    if str(exception_cso_number) in cso_dict.keys():\n",
    "        del cso_dict[str(exception_cso_number)]\n",
    "    return cso_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daef1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_rc(cso_dict, n):\n",
    "    return dict(sorted(cso_dict.items(), key=lambda x: x[1], reverse=True)[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276341fe",
   "metadata": {},
   "source": [
    "### ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27231b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_rouge(cso_dict, actual_rc, actual_rem, type_rpf, n):\n",
    "    max_rc = 0\n",
    "    max_rem = 0\n",
    "    rc_cso = 0\n",
    "    rem_cso = 0\n",
    "    top_dic = get_top_n_rc(cso_dict,n)\n",
    "    for i in top_dic.keys():\n",
    "        foo = cso_df.iloc[cso_df.index[cso_df['cso_number']==int(i)].tolist()[0]]\n",
    "        trc = rouge.get_scores(str(foo['root_cause']), str(actual_rc))[0]['rouge-l'][type_rpf]\n",
    "        if trc < 1 and trc >= max_rc:\n",
    "            rc_cso = i\n",
    "            max_rc = trc\n",
    "        trm = rouge.get_scores(str(foo['remediations']), str(actual_rem))[0]['rouge-l'][type_rpf]\n",
    "        if trm < 1 and trm >= max_rem:\n",
    "            rem_cso = i\n",
    "            max_rem = trm\n",
    "    return max_rc, max_rem, rc_cso, rem_cso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643880f",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ec622",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cso_list = [14757,\n",
    " 6704,\n",
    " 15019,\n",
    " 15126,\n",
    " 8365,\n",
    " 12070,\n",
    " 14965,\n",
    " 6524,\n",
    " 14886,\n",
    " 16742,\n",
    " 9131,\n",
    " 6119,\n",
    " 9144,\n",
    " 15484,\n",
    " 16516,\n",
    " 6894,\n",
    " 13738,\n",
    " 9560,\n",
    " 10190,\n",
    " 7599,\n",
    " 9242,\n",
    " 6920,\n",
    " 17510,\n",
    " 9060,\n",
    " 9828,\n",
    " 15215,\n",
    " 15005,\n",
    " 15558,\n",
    " 12686,\n",
    " 8548,\n",
    " 9139,\n",
    " 7653,\n",
    " 8653,\n",
    " 13678,\n",
    " 15461,\n",
    " 8754,\n",
    " 14055,\n",
    " 10999,\n",
    " 15334,\n",
    " 7872,\n",
    " 9624,\n",
    " 6577,\n",
    " 14886,\n",
    " 14902,\n",
    " 14797,\n",
    " 10384,\n",
    " 10961,\n",
    " 12052,\n",
    " 9563,\n",
    " 6704]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90b649",
   "metadata": {},
   "source": [
    "### Input Type - Description at the time of Outage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2623766",
   "metadata": {},
   "outputs": [],
   "source": [
    "cso_descr = pd.read_csv(\"./CSO_data/cso_alert.csv\")\n",
    "len(cso_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicer(my_str,sub):\n",
    "    index=my_str.find(sub)\n",
    "    if index !=-1 :\n",
    "        return my_str[index+8:] \n",
    "    else :\n",
    "        raise Exception(my_str,' -----Sub string not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee846697",
   "metadata": {},
   "outputs": [],
   "source": [
    "cso_descr_data = []\n",
    "for index, row in cso_descr.iterrows():\n",
    "    z = row['cso_number']\n",
    "    x = row['description']\n",
    "    try:\n",
    "        y = slicer(x,\"<br><br>\")\n",
    "        cso_descr_data.append([z,y])\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "cso_descr_df = pd.DataFrame(cso_descr_data, columns=['cso_number', 'descr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3245ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_rc_rouge = []\n",
    "faiss_rem_rouge = []\n",
    "for cso in random_cso_list:\n",
    "    index = cso_df.index[cso_df['cso_number'] == int(cso)].tolist()[0]\n",
    "    actual_rc = cso_df['root_cause'][index]\n",
    "    actual_rem = cso_df['remediations'][index]\n",
    "\n",
    "    try:\n",
    "        index2 = cso_df.index[cso_df['cso_number'] == cso].tolist()[0]\n",
    "        symptom = cso_df.iloc[index2]['symptom']\n",
    "        cso_dict = get_faiss_rank(symptom)\n",
    "        a,b,c,d = max_rouge(remove_cso_from_dict(cso_dict, int(cso)), actual_rc, actual_rem, 'f', 5)\n",
    "        faiss_rc_rouge.append(a)\n",
    "        faiss_rem_rouge.append(b)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(\"Average of FAISS RC= \", np.mean(faiss_rc_rouge), \"\\nAverage of FAISS Rem= \", np.mean(faiss_rem_rouge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8c666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9386f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
