{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aaf9496-2691-4d7e-9a50-a36be9aecd84",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference & Evaluation for Salesforce Paper "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13876959-877d-477c-a7a2-b5b55c72a4da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d141ceb4-942c-420b-bd97-6e422669265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: progressbar2 in /opt/conda/lib/python3.8/site-packages (4.0.0)\n",
      "Requirement already satisfied: python-utils>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from progressbar2) (3.3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install progressbar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a695be5c-0514-4827-90ef-75afcf96216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import progressbar\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import faiss\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import(word_tokenize, sent_tokenize, TreebankWordTokenizer, wordpunct_tokenize, TweetTokenizer, MWETokenizer)\n",
    "from statistics import mean\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from transformers import pipeline\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4e0e6bd-3a7c-4f47-b256-2189946697cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cso_df = pd.read_csv(\"./CSO_data/CSO_salesforce_extracted_entities.csv\", index_col=0)\n",
    "cso_df = pd.read_csv(\"CSO_data/CSO_entities_ensembled_Sign_bart-large-cnn-samsum.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6fe581ee-65dc-432d-b122-a3d16c085e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = './CSO_data/CSO_all_scraped_Sign.json'\n",
    "with open(json_file, 'r') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f74c053f-299e-4cf3-b726-d8966e791041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "model = RobertaModel.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c2be7e-fa74-4421-ad8b-5e317dd00e5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FAISS Incident Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90f39f0c-fb39-4143-8368-2adf238ceff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sent, d = 1024): ## ignores the words not present in the vocabulary, returns 0 vector in case of empty string or string in which no word has any embedding\n",
    "    res = np.zeros((d,), dtype = 'float32')\n",
    "    count = 0\n",
    "    words = list(word_tokenize(sent))\n",
    "    for word in words:\n",
    "        try:\n",
    "            res = res + embed_directory[word]\n",
    "            count += 1\n",
    "        except:\n",
    "            continue\n",
    "    if (count > 0):\n",
    "        res = res / count\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d6e7005-9edf-48d8-aeda-8cb2406e3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saved Glove embeddings at 'glove.42B.300d/word_embed_0_19174.json'\n",
    "## just load them\n",
    "def split_save_embeddings(data_dict, folder_name, file_generic):\n",
    "    l = len(data_dict.keys())\n",
    "    size = int(l*(0.01))\n",
    "    count = 0\n",
    "    temp = {}\n",
    "    ###### to show progress\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    ###### to show progress\n",
    "    for key in data_dict:\n",
    "        if isinstance(data_dict[key], np.ndarray):\n",
    "            temp[key] = data_dict[key].tolist()\n",
    "        else:\n",
    "            temp[key] = data_dict[key]\n",
    "        count += 1\n",
    "        if (count%size == 0):\n",
    "            file_name = folder_name + '/' + file_generic + '_' +  str(count-size) + '_' + str(count) + '.json'\n",
    "            with open(file_name, 'w') as f:\n",
    "                json.dump(temp, f)\n",
    "            temp = {}\n",
    "        bar.update(count)\n",
    "def load_split_saved_embeddings(folder_name):\n",
    "    global embed_directory\n",
    "    embed_directory = {}\n",
    "    \n",
    "    print('reading files')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    ###### to show progress\n",
    "    l = len(os.listdir(folder_name))\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    count = 0\n",
    "    ###### to show progress\n",
    "    \n",
    "    \n",
    "    for file in os.listdir(folder_name):\n",
    "        if file == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        with open(folder_name + '/' + file , 'r') as f:\n",
    "            embed_directory = {**embed_directory, **json.load(f)}\n",
    "        count += 1\n",
    "        bar.update(count)\n",
    "    \n",
    "    print('converting to numpy array')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    ###### to show progress\n",
    "    l = len(embed_directory.keys())\n",
    "    count = 0\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    ###### to show progress\n",
    "    \n",
    "    for word in embed_directory:\n",
    "        embed_directory[word] = np.asarray(embed_directory[word], dtype = 'float32')\n",
    "        count += 1\n",
    "        bar.update(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19a4fac0-0552-4dce-9cd8-f0d833bbf265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:33:10] |**********************************| (ETA:  00:00:00) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to numpy array\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:35] |**********************************| (ETA:  00:00:00) "
     ]
    }
   ],
   "source": [
    "load_split_saved_embeddings('glove.42B.300d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31f3bf67-ccbd-43b0-b7cb-74f5a96a48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_save_sent_embeddings(folder_name):\n",
    "    # global sent_embed\n",
    "    sent_embed = {}\n",
    "    sent = {}\n",
    "    print('reading files')\n",
    "    sys.stdout.flush()\n",
    "    l = len(os.listdir(folder_name))\n",
    "    count = 0\n",
    "    ###### to show progress\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    ###### to show progress\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        with open(folder_name + '/' + file, 'r') as f:\n",
    "            sent = {**sent, **json.load(f)}\n",
    "        count += 1\n",
    "        bar.update(count)\n",
    "    \n",
    "    print('converting list to numpy array')\n",
    "    sys.stdout.flush()\n",
    "    l = len(sent.keys())\n",
    "    count = 0\n",
    "    ###### to show progress\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    ###### to show progress\n",
    "    for i in sent:\n",
    "        sent_embed[int(i)] = sent[i]\n",
    "        sent_embed[int(i)]['embed'] = np.asarray(sent[i]['embed'], dtype = 'float32')\n",
    "        count += 1\n",
    "        bar.update(count)\n",
    "    sys.stdout.flush()\n",
    "    return sent_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69b7d383-6503-4605-aecb-6e1d14d4eba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:04] |********************************* | (ETA:   0:00:00) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting list to numpy array\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
     ]
    }
   ],
   "source": [
    "## just run this line to load already calculated sentence embeddings in the directory sent_embed\n",
    "sent_embed = load_split_save_sent_embeddings('./FAISS - search/sent_embeddings_sign_ESRO')\n",
    "# sent_embed = load_split_save_sent_embeddings('/FAISS - search/sent_embeddings_dc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc142584-a83c-4a18-913d-726cddae9c5d",
   "metadata": {},
   "source": [
    "#### 2 ) building the Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0fa9fa42-a187-4107-b385-5f913bb8f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numpy_array(s):\n",
    "    nb = len(s.keys())\n",
    "    xb = []\n",
    "    for i, k in enumerate(s.keys()):\n",
    "        xb.append(s[k]['embed'])\n",
    "        \n",
    "    return np.array(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20bebac1-a09d-4558-bc7c-c5dae87cd06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = create_numpy_array(sent_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7fd67db6-4e38-4c67-9b8d-4c83f8cef7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 1024)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4d161c8-b80b-4958-a806-b3538ef7d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(xb):\n",
    "    global xb_normalized\n",
    "    xb_normalized = deepcopy(xb)\n",
    "    faiss.normalize_L2(xb_normalized)\n",
    "    \n",
    "normalize(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a156d05-3376-45eb-9cf9-70c187b08f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1024\n",
    "Index_L2 = faiss.IndexFlatL2(d)\n",
    "Index_IP = faiss.IndexFlatIP(d)\n",
    "\n",
    "\n",
    "Index_L2.add(xb)\n",
    "Index_IP.add(xb_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5165e14-ebd6-4000-809c-c2be357e2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_similar(xq, k, basis = 'both'):\n",
    "    xq_normalized = deepcopy(xq)\n",
    "    faiss.normalize_L2(xq_normalized)\n",
    "    \n",
    "    D_L2, I_L2 = Index_L2.search(xq, k)\n",
    "    D_IP, I_IP = Index_IP.search(xq_normalized, k)\n",
    "    if basis == 'L2':\n",
    "        return D_L2, I_L2\n",
    "    if basis == 'IP':\n",
    "        return D_IP, I_IP\n",
    "    return D_L2, I_L2, D_IP, I_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7611d90d-d67e-4b3d-954b-72dc11afe177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_cso(query):\n",
    "    # split the query into sentences\n",
    "    # assuming text input\n",
    "    \n",
    "    ranked_cso_dict = dict()\n",
    "    \n",
    "    input_ids = torch.tensor(tokenizer.encode(query, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs[0]\n",
    "\n",
    "    token_embeddings = torch.squeeze(last_hidden_states, dim=0)\n",
    "    sentence_embed = torch.mean(token_embeddings, axis=0).numpy().reshape(1,-1)\n",
    "    k = 12\n",
    "        \n",
    "    # print(xq.shape)\n",
    "    D_IP, I_IP = find_top_k_similar(sentence_embed, k, 'IP')  ## shape (nq, k)\n",
    "    # print(D_IP)\n",
    "    for i in range(1):\n",
    "        for j in range(k):\n",
    "            cso_sent_dict = sent_embed[I_IP[i,j]]\n",
    "            if cso_sent_dict['cso'] in ranked_cso_dict.keys():\n",
    "                ranked_cso_dict[cso_sent_dict['cso']]['score'] += D_IP[i, j]\n",
    "                ranked_cso_dict[cso_sent_dict['cso']]['sent'].append(cso_sent_dict['sent'])\n",
    "            else:\n",
    "                temp = {'score': D_IP[i, j], 'cso': cso_sent_dict['cso'], 'sent': [cso_sent_dict['sent']]}\n",
    "                ranked_cso_dict[cso_sent_dict['cso']] = temp\n",
    "    \n",
    "    # print(ranked_cso_dict.items())\n",
    "    return dict(sorted(ranked_cso_dict.items(), key=lambda x: x[1]['score'], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a60afd6-9dd3-48dc-a743-2abc3af3d54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef rank_cso(query):\\n    # split the query into sentences\\n    # assuming text input\\n    \\n    ranked_cso_dict = dict()\\n    list_of_sent = sent_tokenize(query)\\n    k = 20\\n    nq = len(list_of_sent)\\n    xq = np.zeros((nq, 300), dtype = 'float32')\\n    \\n    for i, sent in enumerate(list_of_sent):\\n        sent_embedding = get_embedding(sent).reshape((1,-1))\\n        xq[i,:] = sent_embedding\\n        \\n    # print(xq.shape)\\n    D_IP, I_IP = find_top_k_similar(xq, k, 'IP')  ## shape (nq, k)\\n    # print(D_IP)\\n    for i in range(nq):\\n        for j in range(k):\\n            cso_sent_dict = sent_embed[I_IP[i,j]]\\n            if cso_sent_dict['cso'] in ranked_cso_dict.keys():\\n                ranked_cso_dict[cso_sent_dict['cso']]['score'] += D_IP[i, j]\\n                ranked_cso_dict[cso_sent_dict['cso']]['sent'].append(cso_sent_dict['sent'])\\n            else:\\n                temp = {'score': D_IP[i, j], 'sent': [cso_sent_dict['sent']]}\\n                ranked_cso_dict[cso_sent_dict['cso']] = temp\\n    \\n    # print(ranked_cso_dict.items())\\n    return dict(sorted(ranked_cso_dict.items(), key=lambda x: x[1]['score'], reverse=True))\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def rank_cso(query):\n",
    "    # split the query into sentences\n",
    "    # assuming text input\n",
    "    \n",
    "    ranked_cso_dict = dict()\n",
    "    list_of_sent = sent_tokenize(query)\n",
    "    k = 20\n",
    "    nq = len(list_of_sent)\n",
    "    xq = np.zeros((nq, 300), dtype = 'float32')\n",
    "    \n",
    "    for i, sent in enumerate(list_of_sent):\n",
    "        sent_embedding = get_embedding(sent).reshape((1,-1))\n",
    "        xq[i,:] = sent_embedding\n",
    "        \n",
    "    # print(xq.shape)\n",
    "    D_IP, I_IP = find_top_k_similar(xq, k, 'IP')  ## shape (nq, k)\n",
    "    # print(D_IP)\n",
    "    for i in range(nq):\n",
    "        for j in range(k):\n",
    "            cso_sent_dict = sent_embed[I_IP[i,j]]\n",
    "            if cso_sent_dict['cso'] in ranked_cso_dict.keys():\n",
    "                ranked_cso_dict[cso_sent_dict['cso']]['score'] += D_IP[i, j]\n",
    "                ranked_cso_dict[cso_sent_dict['cso']]['sent'].append(cso_sent_dict['sent'])\n",
    "            else:\n",
    "                temp = {'score': D_IP[i, j], 'sent': [cso_sent_dict['sent']]}\n",
    "                ranked_cso_dict[cso_sent_dict['cso']] = temp\n",
    "    \n",
    "    # print(ranked_cso_dict.items())\n",
    "    return dict(sorted(ranked_cso_dict.items(), key=lambda x: x[1]['score'], reverse=True))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "00dccca0-8723-4dd5-af7a-e9a3a7e6da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faiss_rank(query):\n",
    "    t2 = rank_cso(query)\n",
    "    cso_dict = {}\n",
    "    for i in t2:\n",
    "        cso_dict[int(i)] = t2[i]['score']\n",
    "    return cso_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbecd0c7-5641-416e-9947-e18990ebee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_cso_averaged(query):\n",
    "    # split the query into sentences\n",
    "    # assuming text input\n",
    "    \n",
    "    ranked_cso_dict = dict()\n",
    "    list_of_sent = sent_tokenize(query)\n",
    "    k = 12\n",
    "    nq = len(list_of_sent)\n",
    "    xq = np.zeros((nq, 300), dtype = 'float32')\n",
    "    \n",
    "    for i, sent in enumerate(list_of_sent):\n",
    "        sent_embedding = get_embedding(sent).reshape((1,-1))\n",
    "        xq[i,:] = sent_embedding\n",
    "        \n",
    "    # print(xq.shape)\n",
    "    D_IP, I_IP = find_top_k_similar(xq, k, 'IP')  ## shape (nq, k)\n",
    "    # print(D_IP)\n",
    "    for i in range(nq):\n",
    "        for j in range(k):\n",
    "            cso_sent_dict = sent_embed[I_IP[i,j]]\n",
    "            if cso_sent_dict['cso'] in ranked_cso_dict.keys():\n",
    "                ranked_cso_dict[cso_sent_dict['cso']]['score'] += D_IP[i, j]\n",
    "                ranked_cso_dict[cso_sent_dict['cso']]['sent'].append(cso_sent_dict['sent'])\n",
    "            else:\n",
    "                temp = {'score': D_IP[i, j], 'sent': [cso_sent_dict['sent']]}\n",
    "                ranked_cso_dict[cso_sent_dict['cso']] = temp\n",
    "    \n",
    "    # print(ranked_cso_dict.items())\n",
    "    for cso in ranked_cso_dict:\n",
    "        ranked_cso_dict[cso]['score'] = ranked_cso_dict[cso]['score']/ len(ranked_cso_dict[cso]['sent'])\n",
    "    return dict(sorted(ranked_cso_dict.items(), key=lambda x: x[1]['score'], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b512387-f89e-416d-842b-4f5590d58b42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{14932: 1.9938815,\n",
       " 8005: 0.9974503,\n",
       " 8468: 0.9973313,\n",
       " 13840: 0.9972952,\n",
       " 14449: 0.9971793,\n",
       " 9632: 0.9969669,\n",
       " 10190: 0.9969669,\n",
       " 14385: 0.9969669,\n",
       " 14961: 0.9969669,\n",
       " 12187: 0.9968934,\n",
       " 15873: 0.9968875}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Adobe Sign users in India are facing errors trying to log in. If they login somehow, it is taking too long to get their documents signed.'\n",
    "ans_dict = get_faiss_rank(query)\n",
    "ans_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11707ff-64cb-488e-9f3c-0173689c2a9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GCN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f8a08d88-a900-4d9f-82b5-67b73beb841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = pickle.load(open('./gcn_data/bipartite-ckg.gpickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "61f84da1-be1a-40ae-a578-433eb7ddffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = G.copy()\n",
    "\n",
    "nodelist = list(g.nodes())\n",
    "available_cso = list(cso_df['cso_number'].unique())\n",
    "for n in nodelist:\n",
    "    cso = int(n.split('_')[0])\n",
    "    \n",
    "    if cso not in available_cso:\n",
    "        g.remove_node(n)\n",
    "        \n",
    "A = np.array(list(g.nodes()))\n",
    "A = np.reshape(A, (182,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4cee87a2-26b5-4364-acd5-1805f27ba230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank_list(symptom, all_predictions ):\n",
    "    sym = symptom + '_sym'\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            if A[i,j] == sym:\n",
    "                sym_emb = all_predictions[i, j]\n",
    "                \n",
    "    li = []\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            s = A[i, j]\n",
    "            if s[-2:] == 'rc':\n",
    "                li_sub = [s]\n",
    "                rc_emb = all_predictions[i, j]\n",
    "                li_sub.append(rc_emb)\n",
    "                li.append(li_sub)\n",
    "                \n",
    "    scores_list = []\n",
    "    for sub_list in li:\n",
    "        arr = sub_list[1]\n",
    "        dot = np.dot(sym_emb, arr)\n",
    "        ele = [sub_list[0]]\n",
    "        ele.append(dot)\n",
    "        scores_list.append(ele)\n",
    "        \n",
    "    return scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0735700e-8b15-4bfe-8f07-f512f4d7376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcn_rank(cso_number,embedding_path):\n",
    "    all_predictions = np.load(embedding_path)\n",
    "    t1 = get_rank_list(str(cso_number),all_predictions)\n",
    "    td1 = {}\n",
    "    for i in t1:\n",
    "        td1[int(i[0].replace(\"_rc\",\"\"))] = i[1]\n",
    "    return td1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb75e4-8b49-49b8-94ca-9ad802b4cc40",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b3f4441-2291-43a4-97c0-25a8492df4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_stripper(code):\n",
    "    return BeautifulSoup(code).get_text().replace('\\r',' ').replace('\\xa0',' ').replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e655ce7-79c8-4e9f-8748-d1c108cd56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cso_from_dict(cso_dict1, exception_cso_number):\n",
    "    cso_dict = cso_dict1.copy()\n",
    "    if int(exception_cso_number) in cso_dict.keys():\n",
    "        del cso_dict[int(exception_cso_number)]\n",
    "    if str(exception_cso_number) in cso_dict.keys():\n",
    "        del cso_dict[str(exception_cso_number)]\n",
    "    return cso_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75338269-fb82-43ae-82d8-c1d9d5fbf524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_rc(cso_dict, n):\n",
    "    return dict(sorted(cso_dict.items(), key=lambda x: x[1], reverse=True)[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9d9fd-a51d-4b72-aa79-c89c428daec8",
   "metadata": {},
   "source": [
    "### ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "760834ab-2edd-4079-9785-b12ca5009f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "96d2826f-c0df-480c-a5a5-e6ac67dda45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_rouge(cso_dict, actual_rc, actual_rem, type_rpf, n):\n",
    "    max_rc = 0\n",
    "    max_rem = 0\n",
    "    rc_cso = 0\n",
    "    rem_cso = 0\n",
    "    top_dic = get_top_n_rc(cso_dict,n)\n",
    "    for i in top_dic.keys():\n",
    "        foo = cso_df.iloc[cso_df.index[cso_df['cso_number']==int(i)].tolist()[0]]\n",
    "        trc = rouge.get_scores(str(foo['root_cause']), str(actual_rc))[0]['rouge-l'][type_rpf]\n",
    "        if trc<1 and trc>=max_rc:\n",
    "            rc_cso = i\n",
    "            max_rc = trc\n",
    "        trm = rouge.get_scores(str(foo['remediations']), str(actual_rem))[0]['rouge-l'][type_rpf]\n",
    "        if trm<1 and trm>= max_rem:\n",
    "            rem_cso = i\n",
    "            max_rem = trm\n",
    "    return max_rc, max_rem, rc_cso, rem_cso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f7df12b-16ce-4e07-b70b-ef7931bcaf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_rouge_og(cso_dict, actual_rc, actual_rem, type_rpf, n):\n",
    "    max_rc = 0\n",
    "    max_rem = 0\n",
    "    rc_cso = 0\n",
    "    rem_cso = 0\n",
    "    top_dic = get_top_n_rc(cso_dict,n)\n",
    "    for i in top_dic.keys():\n",
    "        rc = html_stripper(str(json_data[str(i)]['problems'][0]['u_root_cause_description']))\n",
    "        trc = rouge.get_scores(str(rc), str(actual_rc))[0]['rouge-l'][type_rpf]\n",
    "        if trc<1 and trc>=max_rc:\n",
    "            rc_cso = i\n",
    "            max_rc = trc\n",
    "        rem = html_stripper(str(json_data[str(i)]['problems'][0]['u_permanent_solution']))\n",
    "        trm = rouge.get_scores(str(rem), str(actual_rem))[0]['rouge-l'][type_rpf]\n",
    "        if trm<1 and trm>= max_rem:\n",
    "            rem_cso = i\n",
    "            max_rem = trm\n",
    "    return max_rc, max_rem, rc_cso, rem_cso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c55623-ab38-4f10-8635-fe5208700e56",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d67907b5-2d90-4009-816e-a23358e7ffbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_cso_list = [14757,\n",
    " 6704,\n",
    " 15019,\n",
    " 15126,\n",
    " 8365,\n",
    " 12070,\n",
    " 14965,\n",
    " 6524,\n",
    " 14886,\n",
    " 16742,\n",
    " 9131,\n",
    " 6119,\n",
    " 9144,\n",
    " 15484,\n",
    " 16516,\n",
    " 6894,\n",
    " 13738,\n",
    " 9560,\n",
    " 10190,\n",
    " 7599,\n",
    " 9242,\n",
    " 6920,\n",
    " 17510,\n",
    " 9060,\n",
    " 9828,\n",
    " 15215,\n",
    " 15005,\n",
    " 15558,\n",
    " 12686,\n",
    " 8548,\n",
    " 9139,\n",
    " 7653,\n",
    " 8653,\n",
    " 13678,\n",
    " 15461,\n",
    " 8754,\n",
    " 14055,\n",
    " 10999,\n",
    " 15334,\n",
    " 7872,\n",
    " 9624,\n",
    " 6577,\n",
    " 14886,\n",
    " 14902,\n",
    " 14797,\n",
    " 10384,\n",
    " 10961,\n",
    " 12052,\n",
    " 9563,\n",
    " 6704]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72e513-fe6f-4cdd-bd36-5fc6488abb68",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Input Type 1 - Description at the time of Outage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5684185a-aa33-4250-b5f5-93d15b67d673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cso_descr = pd.read_csv(\"./CSO_data/cso_alert.csv\")\n",
    "len(cso_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0252e5fa-0df5-4929-a1c7-918445378ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicer(my_str,sub):\n",
    "    index=my_str.find(sub)\n",
    "    if index !=-1 :\n",
    "        return my_str[index+8:] \n",
    "    else :\n",
    "        raise Exception(my_str,' -----Sub string not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af7d33ae-b19a-4a41-9ca9-5d37da59eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cso_descr_data = []\n",
    "for index, row in cso_descr.iterrows():\n",
    "    #print(z)\n",
    "    z = row['cso_number']\n",
    "    x = row['description']\n",
    "    try:\n",
    "        y = slicer(x,\"<br><br>\")\n",
    "        cso_descr_data.append([z,y])\n",
    "    except:\n",
    "        continue\n",
    "cso_descr_df = pd.DataFrame(cso_descr_data, columns=['cso_number', 'descr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04e9785f-08bb-458b-88e7-37133fac44ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cso_number</th>\n",
       "      <th>descr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17368</td>\n",
       "      <td>Agreements are getting stuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15586</td>\n",
       "      <td>Incident description: Pingdom check of Adobe S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13750</td>\n",
       "      <td>Incident description: Pingdom check of Adobe S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14774</td>\n",
       "      <td>Some customers are unable to login to Adobe Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15558</td>\n",
       "      <td>KBA auth is failing as KBA was down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>16536</td>\n",
       "      <td>S3 Storage Outage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>15666</td>\n",
       "      <td>Issues when sending bulk agreements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>15663</td>\n",
       "      <td>Webhook is not working in HSBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>15007</td>\n",
       "      <td>EU sign in problems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>15005</td>\n",
       "      <td>Adobe Sign EU1 shard is affected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cso_number                                              descr\n",
       "0        17368                       Agreements are getting stuck\n",
       "1        15586  Incident description: Pingdom check of Adobe S...\n",
       "2        13750  Incident description: Pingdom check of Adobe S...\n",
       "3        14774  Some customers are unable to login to Adobe Si...\n",
       "4        15558                KBA auth is failing as KBA was down\n",
       "..         ...                                                ...\n",
       "69       16536                                  S3 Storage Outage\n",
       "70       15666                Issues when sending bulk agreements\n",
       "71       15663                     Webhook is not working in HSBC\n",
       "72       15007                                EU sign in problems\n",
       "73       15005                   Adobe Sign EU1 shard is affected\n",
       "\n",
       "[74 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cso_descr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c320aff7-f6b2-4ae2-82c0-435be441b658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "Average of simpred RC=  0.2023700203374191 \n",
      "Average of simpred Rem=  0.15676940348564283\n"
     ]
    }
   ],
   "source": [
    "simpred_rc_rouge = []\n",
    "simpred_rem_rouge = []\n",
    "for cso in random_cso_list:\n",
    "    index = cso_df.index[cso_df['cso_number'] == int(cso)].tolist()[0]\n",
    "    actual_rc = cso_df['root_cause'][index]\n",
    "    actual_rem = cso_df['remediations'][index]\n",
    "    # actual_rc = html_stripper(str(data[str(cso)]['problems'][0]['u_root_cause_description']))\n",
    "    # actual_rem = html_stripper(str(data[str(cso)]['problems'][0]['u_permanent_solution']))\n",
    "    \n",
    "    #symptom = cso_df['symptom'][index]\n",
    "    try:\n",
    "        index2 = cso_df.index[cso_df['cso_number'] == cso].tolist()[0]\n",
    "        symptom = cso_df.iloc[index2]['symptom']\n",
    "        cso_dict = get_faiss_rank(symptom)\n",
    "        a,b,c,d = max_rouge(remove_cso_from_dict(cso_dict, int(cso)), actual_rc, actual_rem, 'f', 5)\n",
    "        simpred_rc_rouge.append(a)\n",
    "        simpred_rem_rouge.append(b)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(len(simpred_rc_rouge))\n",
    "print(\"Average of simpred RC= \", mean(simpred_rc_rouge), \"\\nAverage of simpred Rem= \", mean(simpred_rem_rouge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61ba1ec7-3419-45e1-8669-5565abc18d1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csos_descr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m actual_rc \u001b[38;5;241m=\u001b[39m html_stripper(\u001b[38;5;28mstr\u001b[39m(json_data[\u001b[38;5;28mstr\u001b[39m(cso)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproblems\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu_root_cause_description\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     12\u001b[0m actual_rem \u001b[38;5;241m=\u001b[39m html_stripper(\u001b[38;5;28mstr\u001b[39m(json_data[\u001b[38;5;28mstr\u001b[39m(cso)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproblems\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu_permanent_solution\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m---> 14\u001b[0m symptom \u001b[38;5;241m=\u001b[39m \u001b[43mcsos_descr\u001b[49m[cso]\n\u001b[1;32m     15\u001b[0m cso_dict \u001b[38;5;241m=\u001b[39m get_faiss_rank(symptom)\n\u001b[1;32m     16\u001b[0m a,b,c,d \u001b[38;5;241m=\u001b[39m max_rouge_og(remove_cso_from_dict(cso_dict,\u001b[38;5;28mint\u001b[39m(cso)),actual_rc, actual_rem,mode,z)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'csos_descr' is not defined"
     ]
    }
   ],
   "source": [
    "mode = 'f'\n",
    "z = 5\n",
    "\n",
    "simpred_rc_rouge = []\n",
    "simpred_rem_rouge = []\n",
    "for cso in random_cso_list:\n",
    "    #print(cso,\" doing\")\n",
    "    index = cso_df.index[cso_df['cso_number']==int(cso)].tolist()[0]\n",
    "    #actual_rc = cso_df['root_cause'][index]\n",
    "    #actual_rem = cso_df['remediations'][index]\n",
    "    actual_rc = html_stripper(str(json_data[str(cso)]['problems'][0]['u_root_cause_description']))\n",
    "    actual_rem = html_stripper(str(json_data[str(cso)]['problems'][0]['u_permanent_solution']))\n",
    "    \n",
    "    symptom = csos_descr[cso]\n",
    "    cso_dict = get_faiss_rank(symptom)\n",
    "    a,b,c,d = max_rouge_og(remove_cso_from_dict(cso_dict,int(cso)),actual_rc, actual_rem,mode,z)\n",
    "    #a,b,c,d = max_rouge(remove_cso_from_dict(cso_dict,int(cso)),actual_rc, actual_rem,mode,z)\n",
    "    simpred_rc_rouge.append(a)\n",
    "    simpred_rem_rouge.append(b)\n",
    "    #print(cso,\" Done\")\n",
    "print(\"Average of simpred RC= \",mean(simpred_rc_rouge),\"\\nAverage of simpred Rem= \",mean(simpred_rem_rouge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e2105-b5c2-463d-8693-d0e672c64f39",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GCN Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "928912fb-f9b8-4b54-a126-84894d62375d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of gcn pred RC=  0.16506580376526678 \n",
      "Average of gcn pred Rem=  0.14748898911550004\n"
     ]
    }
   ],
   "source": [
    "mode = 'f'\n",
    "z = 5\n",
    "embeddings_path = 'gcn_data/gcn_embeddings_5.npy'\n",
    "gcn_rc_rouge = []\n",
    "gcn_rem_rouge = []\n",
    "for cso in random_cso_list:\n",
    "    #print(cso,\" doing\")\n",
    "    index = cso_df.index[cso_df['cso_number']==int(cso)].tolist()[0]\n",
    "    actual_rc = cso_df['root_cause'][index]\n",
    "    actual_rem = cso_df['remediations'][index]\n",
    "    # actual_rc = html_stripper(str(json_data[str(cso)]['problems'][0]['u_root_cause_description']))\n",
    "    # actual_rem = html_stripper(str(json_data[str(cso)]['problems'][0]['u_permanent_solution']))\n",
    "    \n",
    "    cso_dict =  get_gcn_rank(cso, embeddings_path)\n",
    "    a,b,c,d = max_rouge(remove_cso_from_dict(cso_dict, int(cso)),actual_rc, actual_rem,mode,z)\n",
    "    #a,b,c,d = max_rouge(remove_cso_from_dict(cso_dict,int(cso)),actual_rc, actual_rem,mode,z)\n",
    "    gcn_rc_rouge.append(a)\n",
    "    gcn_rem_rouge.append(b)\n",
    "    # except:\n",
    "    #     pass\n",
    "print(\"Average of gcn pred RC= \",mean(gcn_rc_rouge),\"\\nAverage of gcn pred Rem= \",mean(gcn_rem_rouge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7684556-1738-493e-a5a9-72c1233f8833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
