{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13876959-877d-477c-a7a2-b5b55c72a4da",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a695be5c-0514-4827-90ef-75afcf96216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import progressbar\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7cd78-5d3f-4ead-a813-c6d8a17e9273",
   "metadata": {},
   "source": [
    "## Implementation of incident search and RCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4692f-31c9-42c8-ac93-5985867ec258",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1 ) Collecting Entities to create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c038ace9-f172-4c8e-876f-ec95e0b119a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading from a json file, keys are string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06565033-8672-4b2f-9f13-442ace840abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cso_dc_file = './CSO_data/CSO_all_scraped_DC.json'\n",
    "cso_sign_file = './CSO_data/CSO_all_scraped_Sign.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c70bd89c-5ecb-429f-8e34-82f08e2450e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cso_json(file):\n",
    "    global cso_json\n",
    "    with open(file, 'r') as f:\n",
    "        cso_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e219ff02-cf14-405e-9c69-6318a0b127a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using a pretrained embedding model to compute embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7662073b-e149-4412-be62-fd495111a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## dowloading Glove Embeddings\n",
    "# !wget https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "\n",
    "# !unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39aa9c99-f324-4009-8ec2-2f058af27850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## creating embedding directory for later use\n",
    "## jsut load the embeeddings next time\n",
    "def create_embed_directory(file):\n",
    "    global embed_directory\n",
    "    embed_directory = {}\n",
    "    with open(file, encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embed_directory[word] = vector\n",
    "            \n",
    "            \n",
    "def save_embeddings(path):\n",
    "    embed2= {}\n",
    "    for word in embed_directory:\n",
    "        embed2[word] = embed_directory[word].tolist()\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(embed2, f)\n",
    "def load_embeddings(path):\n",
    "    global embed_directory\n",
    "    embed_directory = {}\n",
    "    with open(path, 'r') as f:\n",
    "        embed2 = json.load(f)\n",
    "    for word in embed2:\n",
    "        embed_directory[word] = np.asarray(embed2[word], dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15cee40e-ec74-4d76-97b4-b445cecde8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_embed_directory('glove.42B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c82cad-3f9a-422e-86e1-dd4f5eac1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_embeddings('glove.42B.300d.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3bb02c7-9317-42bc-a71f-52a629187960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_embeddings('glove.42B.300d.json')\n",
    "# for reliability, use load_split_... function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d6e7005-9edf-48d8-aeda-8cb2406e3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saved Glove embeddings at 'glove.42B.300d/word_embed_0_19174.json'\n",
    "## just load them\n",
    "def split_save_embeddings(data_dict, folder_name, file_generic):\n",
    "    l = len(data_dict.keys())\n",
    "    size = int(l*(0.01))\n",
    "    count = 0\n",
    "    temp = {}\n",
    "    ###### to show progress\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    ###### to show progress\n",
    "    for key in data_dict:\n",
    "        if isinstance(data_dict[key], np.ndarray):\n",
    "            temp[key] = data_dict[key].tolist()\n",
    "        else:\n",
    "            temp[key] = data_dict[key]\n",
    "        count += 1\n",
    "        if (count%size == 0):\n",
    "            file_name = folder_name + '/' + file_generic + '_' +  str(count-size) + '_' + str(count) + '.json'\n",
    "            with open(file_name, 'w') as f:\n",
    "                json.dump(temp, f)\n",
    "            temp = {}\n",
    "        bar.update(count)\n",
    "def load_split_saved_embeddings(folder_name):\n",
    "    global embed_directory\n",
    "    embed_directory = {}\n",
    "    \n",
    "    print('reading files')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    ###### to show progress\n",
    "    l = len(os.listdir(folder_name))\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    count = 0\n",
    "    ###### to show progress\n",
    "    \n",
    "    \n",
    "    for file in os.listdir(folder_name):\n",
    "        if file == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        with open(folder_name + '/' + file , 'r') as f:\n",
    "            embed_directory = {**embed_directory, **json.load(f)}\n",
    "        count += 1\n",
    "        bar.update(count)\n",
    "    \n",
    "    print('converting to numpy array')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    ###### to show progress\n",
    "    l = len(embed_directory.keys())\n",
    "    count = 0\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    ###### to show progress\n",
    "    \n",
    "    for word in embed_directory:\n",
    "        embed_directory[word] = np.asarray(embed_directory[word], dtype = 'float32')\n",
    "        count += 1\n",
    "        bar.update(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1daafa-5d44-41a7-9a33-6d8a2175fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_save_embeddings(embed_directory, 'glove.42B.300d', 'word_embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a4fac0-0552-4dce-9cd8-f0d833bbf265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:04:48] |********************************* | (ETA:   0:00:02) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to numpy array\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:32] |**********************************| (ETA:  00:00:00) "
     ]
    }
   ],
   "source": [
    "load_split_saved_embeddings('./FAISS - search/glove.42B.300d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "109337de-b733-474b-9933-d87f1f60ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_cso_json(cso_dc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa1c4674-b55e-4fa4-843f-68a059a20e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import(word_tokenize, sent_tokenize, TreebankWordTokenizer, wordpunct_tokenize, TweetTokenizer, MWETokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90f39f0c-fb39-4143-8368-2adf238ceff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sent, d = 300): ## ignores the words not present in the vocabulary, returns 0 vector in case of empty string or string in which no word has any embedding\n",
    "    res = np.zeros((d,), dtype = 'float32')\n",
    "    count = 0\n",
    "    words = list(word_tokenize(sent))\n",
    "    for word in words:\n",
    "        try:\n",
    "            res = res + embed_directory[word]\n",
    "            count += 1\n",
    "        except:\n",
    "            continue\n",
    "    if (count > 0):\n",
    "        res = res / count\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52e1d097-edaa-41fc-81a0-735676a3f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a new object damn it!\n",
    "## \n",
    "entities1 = ['u_customer_impacts', 'short_description', 'description', 'u_cso_summary', 'u_cso_timeline']\n",
    "entities2 = ['u_permanent_solution', 'u_root_cause_description', 'u_problem_summary', 'u_short_term_fix']\n",
    "def embed_sentences():\n",
    "    global sent_embed\n",
    "    sent_embed = {}\n",
    "    i = 0\n",
    "    ###### to show progress\n",
    "    count = 0\n",
    "    l = len(cso_json.keys())\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    ###### to show progress\n",
    "    \n",
    "    for cso in cso_json:\n",
    "        for entity in entities1:\n",
    "            try:\n",
    "                text = BeautifulSoup(cso_json[cso]['primaryIncident'][entity]).get_text()\n",
    "                text = ' '.join(text.split('\\n'))\n",
    "                text = ' '.join(text.split('\\xa0'))\n",
    "                text = ' '.join(text.split())\n",
    "                text = text.strip()\n",
    "                list_of_sent = list(sent_tokenize(text))\n",
    "                for sent in list_of_sent:\n",
    "                    sent_dict = {'cso':cso, 'sent': '', 'embed': [], 'tag':''}\n",
    "                    try:\n",
    "                        sent_dict['sent'] = sent\n",
    "                        sent_dict['embed'] = get_embedding(sent)\n",
    "                        sent_dict['tag'] = entity\n",
    "                        sent_embed[i] = sent_dict\n",
    "                        i += 1\n",
    "                    except Exception as e:\n",
    "                        print('idhar', e)\n",
    "                        print(sent)\n",
    "                        pass\n",
    "            except Exception as e:\n",
    "                print('bahar1', e)\n",
    "                pass\n",
    "        for entity in entities2:\n",
    "            try:\n",
    "                text = BeautifulSoup(cso_json[cso]['problems'][0][entity]).get_text()\n",
    "                text = ' '.join(text.split('\\n'))\n",
    "                text = ' '.join(text.split('\\xa0'))\n",
    "                text = ' '.join(text.split())\n",
    "                text = text.strip()\n",
    "                list_of_sent = list(sent_tokenize(text))\n",
    "                for sent in list_of_sent:\n",
    "                    sent_dict = {'cso':cso, 'sent': '', 'embed': [], 'tag':''}\n",
    "                    try:\n",
    "                        sent_dict['sent'] = sent\n",
    "                        sent_dict['embed'] = get_embedding(sent)\n",
    "                        sent_dict['tag'] = entity\n",
    "                        sent_embed[i] = sent_dict\n",
    "                        i += 1\n",
    "                    except Exception as e:\n",
    "                        print('udhar', e)\n",
    "                        print(sent)\n",
    "                        pass\n",
    "            except Exception as e:\n",
    "                print('bahar2', e)\n",
    "                pass\n",
    "        count += 1\n",
    "        bar.update(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e99a7d4-9611-4615-8c11-e01b47fb7f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:00] |                                  | (ETA:   0:00:05) /opt/conda/lib/python3.8/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " [elapsed time: 0:00:00] |*                                 | (ETA:   0:00:07) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bahar2 object of type 'NoneType' has no len()\n",
      "bahar2 object of type 'NoneType' has no len()\n",
      "bahar2 object of type 'NoneType' has no len()\n",
      "bahar2 object of type 'NoneType' has no len()\n",
      "bahar2 object of type 'NoneType' has no len()\n",
      "bahar2 object of type 'NoneType' has no len()\n",
      "bahar2 object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:00] |**                                | (ETA:   0:00:07) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bahar1 object of type 'NoneType' has no len()\n",
      "bahar2 object of type 'NoneType' has no len()\n",
      "bahar1 object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:01] |*****                             | (ETA:   0:00:07) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bahar2 object of type 'NoneType' has no len()\n",
      "bahar2 object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:03] |************                      | (ETA:   0:00:06) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bahar2 object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:08] |********************************* | (ETA:   0:00:00) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bahar2 object of type 'NoneType' has no len()\n",
      "bahar2 object of type 'NoneType' has no len()\n"
     ]
    }
   ],
   "source": [
    "embed_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91129440-40be-4d0a-8858-3e0ed52003b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cso': '9670',\n",
       " 'sent': 'For most, if not all of these, the agreement should have woken up when we cleared the poison messages.',\n",
       " 'embed': array([-5.29725216e-02, -3.25934365e-02, -6.27339035e-02, -6.88329563e-02,\n",
       "         5.25446162e-02,  8.45563039e-02, -3.63912392e+00,  4.12863463e-01,\n",
       "         6.68250844e-02, -4.45116937e-01,  4.74666581e-02,  1.30206913e-01,\n",
       "        -1.09079042e-02, -8.51118118e-02,  5.06541058e-02, -1.08435765e-01,\n",
       "        -1.30797178e-01, -6.03506453e-02,  8.97945240e-02, -6.89047202e-02,\n",
       "        -1.94520317e-02, -1.06669143e-01,  4.63088676e-02,  7.18541369e-02,\n",
       "        -1.29691154e-01, -3.30353156e-02, -4.28823829e-02, -1.74532488e-01,\n",
       "        -4.82032858e-02,  5.49510401e-03, -2.38686681e-01,  8.90693367e-02,\n",
       "         6.61032572e-02, -1.34959623e-01, -2.00491883e-02, -9.97144282e-02,\n",
       "        -1.49898857e-01, -1.31189287e-01, -5.34908548e-02, -1.24764614e-01,\n",
       "         5.39006963e-02,  2.16018677e-01, -6.58996552e-02, -1.39826894e-01,\n",
       "        -2.74278801e-02,  5.12788594e-02, -2.77906153e-02, -1.40828341e-01,\n",
       "        -2.57985741e-01,  7.13114291e-02,  2.40702908e-02, -1.84800960e-02,\n",
       "         4.23485739e-03, -6.12462759e-02, -2.66196616e-02, -8.97343308e-02,\n",
       "         8.85951445e-02,  1.61449201e-02,  5.15490351e-03, -9.24800150e-03,\n",
       "         8.28386769e-02,  8.00528005e-02,  9.94779728e-03,  6.77017942e-02,\n",
       "        -8.41009617e-03,  2.83178866e-01,  5.20411953e-02,  7.38447160e-02,\n",
       "         1.17650047e-01,  1.42383333e-02, -2.10746765e-01,  8.29077438e-02,\n",
       "         7.45690018e-02,  4.08340283e-02,  2.34693792e-02, -1.65586248e-01,\n",
       "         1.54835552e-01,  1.31147236e-01,  4.49359529e-02, -1.38608262e-01,\n",
       "         3.41791920e-02, -5.99243820e-01, -1.14761479e-01, -1.38526827e-01,\n",
       "         5.98192289e-02, -3.72585356e-02,  4.79538552e-02, -5.44866500e-03,\n",
       "         6.76191470e-04, -1.44764274e-01,  2.35059764e-02, -1.05964094e-01,\n",
       "        -7.24103600e-02,  6.08197860e-02,  6.83124289e-02, -3.18237811e-01,\n",
       "        -2.19805479e+00, -9.38901901e-02,  1.37735054e-01,  6.31256178e-02,\n",
       "        -7.27588981e-02,  1.32661939e-01,  7.28673190e-02,  5.06659113e-02,\n",
       "        -1.16539396e-01, -2.18419195e-03,  1.03897247e-02, -3.24010909e-01,\n",
       "        -1.23550937e-01, -6.22673333e-02, -1.16982833e-02, -1.15813827e-02,\n",
       "         9.18741375e-02,  1.14878915e-01, -1.45449668e-01,  8.81244391e-02,\n",
       "         2.16645878e-02,  2.00768653e-02,  3.41930464e-02,  1.07690662e-01,\n",
       "        -3.11498493e-02,  1.33913355e-02,  7.03045204e-02, -1.17190972e-01,\n",
       "        -2.51422748e-02, -4.82493825e-02,  9.92529541e-02, -2.11747438e-01,\n",
       "        -5.75828739e-02,  1.95917130e-01, -1.58920214e-01,  1.85647781e-03,\n",
       "        -9.31323916e-02, -8.09655041e-02,  1.07502520e-01,  4.76929471e-02,\n",
       "         1.03728853e-01,  9.50798765e-02,  1.67865053e-01,  2.76243806e-01,\n",
       "         1.08611345e-01,  1.36157721e-01,  8.40526968e-02,  5.90968989e-02,\n",
       "         4.70318273e-02, -1.89202756e-01, -1.27240010e-02, -1.12262614e-01,\n",
       "         3.99519026e-01,  1.34041831e-01,  1.84067637e-02,  4.82821912e-02,\n",
       "         1.30790491e-02,  1.82322301e-02,  5.28140133e-03, -8.85279030e-02,\n",
       "        -6.48535788e-02, -3.55486795e-02, -1.47366121e-01,  8.19921419e-02,\n",
       "        -4.98623289e-02, -5.98370954e-02, -1.41734183e-01, -5.48885763e-02,\n",
       "         2.74655186e-02,  2.73831487e-02,  3.78440134e-02,  9.97523428e-04,\n",
       "         1.09931380e-01, -1.63740396e-01, -1.11068757e-02,  1.33195609e-01,\n",
       "        -1.67028263e-01,  1.78754136e-01, -1.78183585e-01,  2.73640871e-01,\n",
       "        -1.82812847e-02, -1.30331321e-02,  1.09081179e-01, -1.95605829e-01,\n",
       "         9.13849548e-02,  9.86715034e-02,  4.68542874e-02, -8.34902897e-02,\n",
       "        -1.27962813e-01, -1.42172083e-01,  6.29613847e-02,  3.34244780e-02,\n",
       "         1.22032963e-01, -2.64577493e-02,  1.45411998e-01, -8.35283622e-02,\n",
       "         7.80983344e-02, -9.00810212e-02,  3.97256725e-02, -7.05975220e-02,\n",
       "         1.54833123e-01,  3.12323347e-02, -1.03444299e-02, -9.39110368e-02,\n",
       "         1.53162137e-01, -1.69078149e-02, -1.60670891e-01,  1.48169026e-02,\n",
       "        -1.00442488e-03,  1.51219249e-01, -9.60934758e-02,  3.82777639e-02,\n",
       "         2.11512312e-01, -1.85098220e-02,  1.76727787e-01,  1.09346993e-01,\n",
       "        -2.83871777e-02, -9.62214619e-02,  4.65894826e-02, -1.80282768e-05,\n",
       "        -1.41897470e-01, -1.14035934e-01, -5.59473373e-02, -6.51564077e-02,\n",
       "        -3.16458531e-02, -1.65196404e-01,  9.79974587e-03, -5.41901365e-02,\n",
       "        -2.75271964e+00,  1.76905707e-01,  5.10069542e-02,  8.01899061e-02,\n",
       "         4.61890958e-02, -2.25080237e-01,  2.30705217e-02,  8.07854757e-02,\n",
       "         1.32329047e-01, -9.31342691e-02, -6.01088479e-02,  7.12735876e-02,\n",
       "         3.47840488e-02,  9.23599824e-02, -7.80852661e-02,  6.09726571e-02,\n",
       "        -1.51457995e-01,  3.61121334e-02, -1.80959821e-01,  1.29011897e-02,\n",
       "         7.10019330e-03, -1.62932292e-01, -6.50333017e-02,  9.32087079e-02,\n",
       "        -7.72831291e-02,  1.32047892e-01, -6.89124092e-02, -7.79275522e-02,\n",
       "        -7.74076134e-02, -5.46428151e-02,  2.31126193e-02, -1.04205810e-01,\n",
       "         1.02141336e-01, -7.66860098e-02, -7.08759949e-02,  5.58105297e-02,\n",
       "        -1.18212573e-01,  6.18474893e-02,  6.38916120e-02, -5.04780672e-02,\n",
       "         1.49991035e-01,  1.20298639e-02, -1.43391535e-01, -1.30940661e-01,\n",
       "         5.27042560e-02,  3.04894410e-02, -2.13857032e-02, -1.20778039e-01,\n",
       "        -1.86232299e-01, -4.63238582e-02, -1.01824246e-01,  5.63046858e-02,\n",
       "         2.95378026e-02,  1.86649874e-01,  1.26777217e-01, -1.24591134e-01,\n",
       "         4.21658099e-01, -7.96857662e-03,  1.02536574e-01,  1.52540848e-01,\n",
       "        -1.82530105e-01,  1.09553561e-01, -7.87042901e-02, -1.06315047e-01,\n",
       "        -1.53234899e-02,  5.74509539e-02, -1.18892835e-02, -1.16197757e-01,\n",
       "        -1.00954816e-01, -1.89450771e-01, -2.02659853e-02, -2.02464294e-02,\n",
       "        -4.89371382e-02, -1.34504616e-01,  8.74072164e-02,  3.33211967e-03],\n",
       "       dtype=float32),\n",
       " 'tag': 'u_root_cause_description'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_embed[25043]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8721d16a-e173-4b03-ad82-2204bf4dc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sent_embeddings(path):\n",
    "    sent_ = {}\n",
    "    for i in sent_embed:\n",
    "        sent_[i] = sent_embed[i]\n",
    "        sent_[i]['embed'] = sent_embed['embed'].tolist()\n",
    "    with open(path, 'r') as f:\n",
    "        json.dump(sent_,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76dc3330-e351-4bf1-ad23-076137c5b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_embeddings('sent_embeddings_sign.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "491dd4e9-b822-429f-b60d-aadd42b66960",
   "metadata": {},
   "outputs": [],
   "source": [
    "## just load it the next time, already saved at 'sent_embeddings_sign/........'\n",
    "def split_save_sent_embeddings(data_dict, folder_name, file_generic):\n",
    "    l = len(data_dict.keys())\n",
    "    size = int(l*(0.01))\n",
    "    count = 0\n",
    "    temp = {}\n",
    "    ###### to show progress\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    ###### to show progress\n",
    "    for key in data_dict:\n",
    "        temp[key] = data_dict[key]\n",
    "        if isinstance(data_dict[key]['embed'], np.ndarray):\n",
    "            temp[key]['embed'] = data_dict[key]['embed'].tolist()\n",
    "        else:\n",
    "            temp[key] = data_dict[key]\n",
    "        count += 1\n",
    "        if (count%size == 0):\n",
    "            file_name = folder_name + '/' + file_generic + '_' +  str(count-size) + '_' + str(count) + '.json'\n",
    "            with open(file_name, 'w') as f:\n",
    "                json.dump(temp, f)\n",
    "            temp = {}\n",
    "        bar.update(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fd99096-739f-4de6-80a6-bd3eb66b7fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:14] |********************************* | (ETA:   0:00:00) "
     ]
    }
   ],
   "source": [
    "split_save_sent_embeddings(sent_embed, './FAISS - search/sent_embeddings_dc', 'sent_embed')\n",
    "# already saved, load the same as save or save different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "512f93f0-7dac-45c1-8074-4df1314a2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sent_embeddings(path):\n",
    "    global sent_embed\n",
    "    sent_embed = {}\n",
    "    with open(path, 'r') as f:\n",
    "        sent_embed = json.load(f)\n",
    "    l = len(sent_embed.keys())\n",
    "    count = 0\n",
    "    ###### to show progress\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    ###### to show progress\n",
    "    for i in sent_embed:\n",
    "        # sent_embed[i] = sent_[i]\n",
    "        sent_embed[i]['embed'] = np.asarray(sent_embed[i]['embed'], dtype = 'float32')\n",
    "        count += 1\n",
    "        bar.update(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd85c81c-8199-486c-b1a3-7641f4da7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_sent_embeddings('sent_embeddings_sign.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31f3bf67-ccbd-43b0-b7cb-74f5a96a48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_save_sent_embeddings(folder_name):\n",
    "    # global sent_embed\n",
    "    sent_embed = {}\n",
    "    sent = {}\n",
    "    print('reading files')\n",
    "    sys.stdout.flush()\n",
    "    l = len(os.listdir(folder_name))\n",
    "    count = 0\n",
    "    ###### to show progress\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    ###### to show progress\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        with open(folder_name + '/' + file, 'r') as f:\n",
    "            sent = {**sent, **json.load(f)}\n",
    "        count += 1\n",
    "        bar.update(count)\n",
    "    \n",
    "    print('converting list to numpy array')\n",
    "    sys.stdout.flush()\n",
    "    l = len(sent.keys())\n",
    "    count = 0\n",
    "    ###### to show progress\n",
    "    widgets = [' [', progressbar.Timer(format= 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('*'),' (', progressbar.ETA(), ') ', ]\n",
    "    bar = progressbar.ProgressBar(max_value=l, widgets=widgets).start()\n",
    "    ###### to show progress\n",
    "    for i in sent:\n",
    "        sent_embed[int(i)] = sent[i]\n",
    "        sent_embed[int(i)]['embed'] = np.asarray(sent[i]['embed'], dtype = 'float32')\n",
    "        count += 1\n",
    "        bar.update(count)\n",
    "    sys.stdout.flush()\n",
    "    return sent_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69b7d383-6503-4605-aecb-6e1d14d4eba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:04] |********************************* | (ETA:   0:00:00) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting list to numpy array\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:00] |***************************       | (ETA:   0:00:00) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:05] |********************************* | (ETA:   0:00:00) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting list to numpy array\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:00] |******************************    | (ETA:   0:00:00) "
     ]
    }
   ],
   "source": [
    "## just run this line to load already calculated sentence embeddings in the directory sent_embed\n",
    "sent_embed_sign = load_split_save_sent_embeddings('./FAISS - search/sent_embeddings_sign')\n",
    "sent_embed = load_split_save_sent_embeddings('./FAISS - search/sent_embeddings_dc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc142584-a83c-4a18-913d-726cddae9c5d",
   "metadata": {},
   "source": [
    "#### 2 ) building the Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fa9fa42-a187-4107-b385-5f913bb8f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_numpy_array(sent_embed):\n",
    "    nb = len(sent_embed.keys())\n",
    "    d = 300 ## the dimension of the embeddings\n",
    "    global xb\n",
    "    xb = np.zeros((nb,d), dtype = 'float32')\n",
    "    for i in range(nb):\n",
    "        xb[i,:] = sent_embed[i]['embed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20bebac1-a09d-4558-bc7c-c5dae87cd06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_numpy_array(sent_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4d161c8-b80b-4958-a806-b3538ef7d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(xb):\n",
    "    global xb_normalized\n",
    "    xb_normalized = deepcopy(xb)\n",
    "    faiss.normalize_L2(xb_normalized)\n",
    "    \n",
    "normalize(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a156d05-3376-45eb-9cf9-70c187b08f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300\n",
    "Index_L2 = faiss.IndexFlatL2(d)\n",
    "Index_IP = faiss.IndexFlatIP(d)\n",
    "\n",
    "\n",
    "Index_L2.add(xb)\n",
    "Index_IP.add(xb_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5165e14-ebd6-4000-809c-c2be357e2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_similar(xq, k, basis = 'both'):\n",
    "    xq_normalized = deepcopy(xq)\n",
    "    faiss.normalize_L2(xq_normalized)\n",
    "    \n",
    "    D_L2, I_L2 = Index_L2.search(xq, k)\n",
    "    D_IP, I_IP = Index_IP.search(xq_normalized, k)\n",
    "    if basis == 'L2':\n",
    "        return D_L2, I_L2\n",
    "    if basis == 'IP':\n",
    "        return D_IP, I_IP\n",
    "    return D_L2, I_L2, D_IP, I_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a60afd6-9dd3-48dc-a743-2abc3af3d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_cso(query):\n",
    "    # split the query into sentences\n",
    "    # assuming text input\n",
    "    \n",
    "    ranked_cso_dict = dict()\n",
    "    list_of_sent = sent_tokenize(query)\n",
    "    k = 12\n",
    "    nq = len(list_of_sent)\n",
    "    xq = np.zeros((nq, d), dtype = 'float32')\n",
    "    \n",
    "    for i, sent in enumerate(list_of_sent):\n",
    "        sent_embedding = get_embedding(sent).reshape((1,-1))\n",
    "        xq[i,:] = sent_embedding\n",
    "        \n",
    "    # print(xq.shape)\n",
    "    D_IP, I_IP = find_top_k_similar(xq, k, 'IP')  ## shape (nq, k)\n",
    "    # print(D_IP)\n",
    "    for i in range(nq):\n",
    "        for j in range(k):\n",
    "            cso_sent_dict = sent_embed[I_IP[i,j]]\n",
    "            if cso_sent_dict['cso'] in ranked_cso_dict.keys():\n",
    "                ranked_cso_dict[cso_sent_dict['cso']]['score'] += D_IP[i, j]\n",
    "                ranked_cso_dict[cso_sent_dict['cso']]['sent'].append(cso_sent_dict['sent'])\n",
    "            else:\n",
    "                temp = {'score': D_IP[i, j], 'sent': [cso_sent_dict['sent']]}\n",
    "                ranked_cso_dict[cso_sent_dict['cso']] = temp\n",
    "    \n",
    "    # print(ranked_cso_dict.items())\n",
    "    return dict(sorted(ranked_cso_dict.items(), key=lambda x: x[1]['score'], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8427436c-857e-4a95-84b3-9ca6b3274178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'16649': {'score': 2.903912,\n",
       "  'sent': ['Adobe Sign users are unable to log in.',\n",
       "   'Adobe Sign users are unable to log in',\n",
       "   'Between 2022-01-18 and 2022-02-10 at 23:03 UTC, some users attempting to log in to Adobe Sign were unable to.']},\n",
       " '14932': {'score': 2.8971612,\n",
       "  'sent': ['On 2021-07-15 between 19:04 and 22:10 UTC, Outlook/WEP users attempting to login to Adobe Sign using the Office 365 integration were unable to sign in, preventing the users from accessing Adobe Sign.When attempting to log in the users would have experienced the error message “Sorry, an unexpected error occurred.',\n",
       "   'Impact: On 2021-07-15 between 19:04 and 22:10 UTC, Outlook/WEP users attempting to login to Adobe Sign using the Office 365 integration were unable to sign in, preventing the users from accessing Adobe Sign.When attempting to log in the users would have experienced the error message “Sorry, an unexpected error occurred.',\n",
       "   'It is not logged either on our end when we make the request to sign, or on sign’s end when they receive the request, so it’s not possible to definitively determine which one of those possibilities did in fact occur.']},\n",
       " '14536': {'score': 1.9491235,\n",
       "  'sent': [\"But looks like we didn't get a chance to do any deployments between when it renewed and today 15:19 , though, the previous search was still in place, which seemed like they may have expired today, but we just completed the deploy, which could pick up the new certificate's 15:29 just so it's just we didn't get it.\",\n",
       "   'On 2021-05-26 between 19:35 UTC and 20:21 UTC, some customers attempting to upload documents using the PDFnow service would have seen a \"something went wrong\" error message and would not have been able to successfully complete their upload.']},\n",
       " '15235': {'score': 1.9223088,\n",
       "  'sent': ['20:56 Performance is slow but users are still able to log in.',\n",
       "   'There are still some errors in the Redis logs.']},\n",
       " '17401': {'score': 1.9195111,\n",
       "  'sent': ['On 2022-05-06, between 07:19 UTC and 07:36 UTC, some North America Adobe Sign users might have been unable to log in to the Adobe Sign site or encountered delays in document signing.',\n",
       "   'On 2022-05-06, between 07:19 UTC and 07:36 UTC, some North America Adobe Sign users might have been unable to log in to the Adobe Sign site or encountered delays in document signing.']},\n",
       " '6165': {'score': 0.9784125,\n",
       "  'sent': ['This would have prevented the CSO today, but it does leave you with your change not being complete, so we’d need to retry periodically to get the change through.']},\n",
       " '10597': {'score': 0.97637343,\n",
       "  'sent': ['7:40 AM Jess: The PDF will be forever locked with the set password, if users forget it, they will not be able to access the agreement after it’s been signed.']},\n",
       " '10320': {'score': 0.9750298,\n",
       "  'sent': [\"Unfortunately, we haven't figured out how to get into the situation where a conversation in teams chat has some members with empty object ids, so we can't attempt to reproduce it.There have been no recent changes, so it's been this way for a couple of years.\"]},\n",
       " '6572': {'score': 0.9744759,\n",
       "  'sent': [\"I wouldn't put all the stuff about yesterday in, but you should send this one now to get it out.\"]},\n",
       " '9126': {'score': 0.9736476,\n",
       "  'sent': ['You can check back later to see if it has succeeded at your list of shared files.9:31 AM The messages are not getting queued because they are encrypted.9:32 AM Manoj Bhatt joined the call.']},\n",
       " '17088': {'score': 0.97292364,\n",
       "  'sent': ['00:38 Dean can sign the document, but it took much longer than expected.']},\n",
       " '10658': {'score': 0.9721812,\n",
       "  'sent': [\"Between 8:15 AM and 8:41 AM PT, some Adobe Sign customers attempting to log in through the secure site (secure.echosign.com) would have seen an incorrect branding logo for the Provident Bank on the sign in page, even though they didn't relate to said customer.This didn't affect any customer's ability to log in and use Adobe Sign.\"]},\n",
       " '7872': {'score': 0.9720947,\n",
       "  'sent': [\"I don't think most customers know their shard - we can't assume they do, anyway.Arun Mehra: got it ThanksDave Owczarek: As you are probably aware, there are some folks trying to build out a feature to send messages only to the contacts from specific shards, but I don't believe it's ready yet.\"]},\n",
       " '13620': {'score': 0.9719218,\n",
       "  'sent': ['18:12 - Impact to PDF now would be mostly complete some small files might get through but since its a capacity issue its hard to say.']},\n",
       " '10601': {'score': 0.95976627,\n",
       "  'sent': ['Customers trying to secure log in are incorrectly redirected to the Adobe ID page.']},\n",
       " '17780': {'score': 0.95865583,\n",
       "  'sent': ['Users from Microsoft are unable to log in']},\n",
       " '8775': {'score': 0.9579984,\n",
       "  'sent': ['3:50 PM Catalin running some queries to get a count of users trying to log in with Google.']}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Adobe Sign users in India are facing errors trying to log in. If they login somehow, it is taking too long to get their documents signed.'\n",
    "ans_dict = rank_cso(query)\n",
    "ans_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b434c2-1928-4a05-94cd-bd3fa1046249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
